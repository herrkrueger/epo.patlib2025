{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f70373-ddfd-4c86-acde-df1812b10ab1",
   "metadata": {},
   "source": [
    "### si tratta di riprodurre alcuni risultati giÃ  trovati con Patstat online tramite SQLAlchemy. Alcune strategie sono comunque utili per analisi di Patent Intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc6699f-b7ae-4171-91a0-9f818dd85a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the patstat client\n",
    "from epo.tipdata.patstat import PatstatClient\n",
    "\n",
    "# Initialize the PATSTAT client\n",
    "patstat = PatstatClient(env='PROD')\n",
    "\n",
    "# Access ORM\n",
    "db = patstat.orm()\n",
    "\n",
    "# Importing tables as models\n",
    "from epo.tipdata.patstat.database.models import TLS201_APPLN,TLS203_APPLN_ABSTR, TLS202_APPLN_TITLE, TLS209_APPLN_IPC, TLS224_APPLN_CPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6db99-704a-4ed8-be78-8e64d5145be6",
   "metadata": {},
   "source": [
    "# Ora trattiamo un caso reale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473aae4-11a8-437f-928a-5933c3353d3d",
   "metadata": {},
   "source": [
    "Step 1: Retrieve distinct docdb_family_id values corresponding to keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea05a695-64e5-4d0b-95ab-45fdfcfc2d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct docdb_family_id values: 84905\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import and_, or_\n",
    "# Keywords for the abstract and title\n",
    "keywords = [\n",
    "    \"rare earth element*\", \"light REE*\", \"heavy REE*\", \"rare earth metal*\",\n",
    "    \"rare earth oxide*\", \"lanthan*\", \"rare earth\"\n",
    "]\n",
    "# Step 1: Retrieve distinct docdb_family_id values corresponding to keywords\n",
    "subquery_keywords = (\n",
    "    db.query(TLS201_APPLN.docdb_family_id)\n",
    "    .join(TLS203_APPLN_ABSTR, TLS203_APPLN_ABSTR.appln_id == TLS201_APPLN.appln_id)\n",
    "    .filter(or_(*[TLS203_APPLN_ABSTR.appln_abstract.contains(kw) for kw in keywords]))\n",
    "    .union(\n",
    "        db.query(TLS201_APPLN.docdb_family_id)\n",
    "        .join(TLS202_APPLN_TITLE, TLS202_APPLN_TITLE.appln_id == TLS201_APPLN.appln_id)\n",
    "        .filter(or_(*[TLS202_APPLN_TITLE.appln_title.contains(kw) for kw in keywords]))\n",
    "    ).distinct()\n",
    ").all()\n",
    "\n",
    "# Convert result to a list of docdb_family_id values\n",
    "docdb_family_ids_keywords = [row.docdb_family_id for row in subquery_keywords]\n",
    "# Print the number of distinct docdb_family_id values\n",
    "print(f\"Number of distinct docdb_family_id values: {len(docdb_family_ids_keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef67940-bf0c-4f2e-8023-718c1d7e2371",
   "metadata": {},
   "source": [
    "Step 2: Retrieve distinct docdb_family_id values corresponding to classification codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b45a990-0b8a-42e8-8a62-b1b4fa704768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results in Step 2: 567012\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import func\n",
    "\n",
    "# Classification codes with varying lengths\n",
    "\n",
    "ipc_codes_11 = [\n",
    "'A43B   1/12','B03B   9/06','B29B   7/66','B30B   9/32','B65D  65/46','C03B   1/02',\n",
    "'C04B   7/24','C04B   7/26','C04B   7/28','C04B   7/30','C04B  11/26','C04B  18/04','C04B  18/06','C04B  18/08','C04B  18/10',\n",
    "'C04B  18/12','C04B  18/14','C04B  18/16','C04B  18/18','C04B  18/20','C04B  18/22','C04B  18/24','C04B  18/26','C04B  18/28',\n",
    "'C04B  18/30','C09K  11/01','C22B  19/28','C22B  19/30','C22B  25/06','D21B   1/08','D21B   1/10','D21B   1/32','D21C   5/02',\n",
    "'D21H  17/01','H01B  15/00','H01J   9/52','H01M   6/52','H01M  10/54']\n",
    "\n",
    "ipc_codes_8 = [\n",
    "'B22F   8','B29B  17','B62D  67','B65H  73',\n",
    "'C08J  11','C10M 175','C22B   7','D01G  11']\n",
    "\n",
    "ipc_codes_12 = [\n",
    "'C04B  33/132']\n",
    "\n",
    "cpc_codes_11 = ['A43B   1/12','B03B   9/06','B29B   7/66','B30B   9/32','B65D  65/46','C03B   1/02',\n",
    "'C04B   7/24','C04B   7/26','C04B   7/28','C04B   7/30','C04B  11/26','C04B  18/04','C04B  18/06','C04B  18/08','C04B  18/10',\n",
    "'C04B  18/12','C04B  18/14','C04B  18/16','C04B  18/18','C04B  18/20','C04B  18/22','C04B  18/24','C04B  18/26','C04B  18/28',\n",
    "'C04B  18/30','C09K  11/01','C22B  19/28','C22B  19/30','C22B  25/06','D21B   1/08','D21B   1/10','D21B   1/32','D21C   5/02',\n",
    "'D21H  17/01','H01B  15/00','H01J   9/52','H01M   6/52','H01M  10/54','Y02W  30/50','Y02W  30/52','Y02W  30/56','Y02W  30/58',\n",
    "'Y02W  30/60','Y02W  30/62','Y02W  30/64','Y02W  30/66','Y02W  30/74','Y02W  30/78','Y02W  30/80','Y02W  30/82','Y02W  30/84',\n",
    "'Y02W  30/91','Y02P  10/20']\n",
    "\n",
    "cpc_codes_8 = ['B22F   8','B29B  17','B62D  67','B65H  73',\n",
    "'C08J  11','C10M 175','C22B   7','D01G  11']\n",
    "\n",
    "cpc_codes_12 = ['C04B  18/068','C04B  33/132',\n",
    "'C04B   7/243','C04B   7/246','C04B  18/049','C04B  18/061','C04B  18/062','C04B  18/064','C04B  18/065',\n",
    "'C04B  18/067','C04B  18/081','C04B  18/082','C04B  18/084','C04B  18/085','C04B  18/087','C04B  18/088',\n",
    "'C04B  18/101','C04B  18/103','C04B  18/105','C04B  18/106','C04B  18/108','C04B  18/125',\n",
    "'C04B  18/141','C04B  18/142','C04B  18/143','C04B  18/144','C04B  18/145','C04B  18/146','C04B  18/147',\n",
    "'C04B  18/148','C04B  18/149','C04B  18/162','C04B  18/165','C04B  18/167','C04B  18/241','C04B  18/243',\n",
    "'C04B  18/245','C04B  18/246','C04B  18/248','C04B  18/265','C04B  18/305']\n",
    "\n",
    "\n",
    "cpc_codes_13 = ['C04B  18/0409','C04B  18/0418',\n",
    "'C04B  18/0427','C04B  18/0436','C04B  18/0445','C04B  18/0454','C04B  18/0463','C04B  18/0472','C04B  18/0481']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Subquery for classification codes with varying lengths\n",
    "subquery_classcodes = (\n",
    "    db.query(TLS201_APPLN.docdb_family_id)\n",
    "    .join(TLS209_APPLN_IPC, TLS209_APPLN_IPC.appln_id == TLS201_APPLN.appln_id)\n",
    "    .filter(\n",
    "        or_(\n",
    "            func.substr(TLS209_APPLN_IPC.ipc_class_symbol, 1, 11).in_(ipc_codes_11),\n",
    "            func.substr(TLS209_APPLN_IPC.ipc_class_symbol, 1, 8).in_(ipc_codes_8),\n",
    "            func.substr(TLS209_APPLN_IPC.ipc_class_symbol, 1, 12).in_(ipc_codes_12)\n",
    "        )\n",
    "    )\n",
    "    .union(\n",
    "        db.query(TLS201_APPLN.docdb_family_id)\n",
    "        .join(TLS224_APPLN_CPC, TLS224_APPLN_CPC.appln_id == TLS201_APPLN.appln_id)\n",
    "        .filter(\n",
    "            or_(\n",
    "                func.substr(TLS224_APPLN_CPC.cpc_class_symbol, 1, 11).in_(cpc_codes_11),\n",
    "                func.substr(TLS224_APPLN_CPC.cpc_class_symbol, 1, 8).in_(cpc_codes_8),\n",
    "                func.substr(TLS224_APPLN_CPC.cpc_class_symbol, 1, 12).in_(cpc_codes_12),\n",
    "                func.substr(TLS224_APPLN_CPC.cpc_class_symbol, 1, 12).in_(cpc_codes_13)\n",
    "            )\n",
    "        )\n",
    "    ).distinct()\n",
    ").all()\n",
    "\n",
    "# Convert result to a list of docdb_family_id values\n",
    "docdb_family_ids_classcodes = [row.docdb_family_id for row in subquery_classcodes]\n",
    "\n",
    "# Print the number of results\n",
    "print(f\"Number of results in Step 2: {len(docdb_family_ids_classcodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee06a7-7aa1-451c-8bc7-d3a296541ecb",
   "metadata": {},
   "source": [
    "Step 3: Get the intersection of the two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8735a862-f258-4116-9473-54c87d5c9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get the intersection of the two lists\n",
    "intersection_docdb_family_ids = list(set(docdb_family_ids_keywords) & set(docdb_family_ids_classcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3727eb4-8f5e-4d68-b88e-10ae778c6526",
   "metadata": {},
   "source": [
    "Step 4: Retrieve and display the list of results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b53f2d7d-ec60-4ade-97e9-37df759db4d1",
   "metadata": {},
   "source": [
    "from sqlalchemy.orm import aliased\n",
    "from sqlalchemy import and_, func\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the necessary imports for your database models\n",
    "from epo.tipdata.patstat.database.models import TLS209_APPLN_IPC, TLS201_APPLN\n",
    "\n",
    "# Alias TLS209_APPLN_IPC to avoid confusion\n",
    "TLS209_APPLN_IPC_2 = aliased(TLS209_APPLN_IPC)\n",
    "\n",
    "# Step 4: Filter the data according to the timeframe 2010 - 2022 and join with TLS209_APPLN_IPC\n",
    "final_query = (\n",
    "    db.query(\n",
    "        TLS201_APPLN.docdb_family_id.label('patent_family_id'),\n",
    "        TLS201_APPLN.earliest_filing_year.label('earliest_filing_year'),\n",
    "        TLS209_APPLN_IPC.ipc_class_symbol.label('IPC_1'),\n",
    "        TLS209_APPLN_IPC_2.ipc_class_symbol.label('IPC_2')\n",
    "    )\n",
    "    .join(TLS209_APPLN_IPC, TLS201_APPLN.appln_id == TLS209_APPLN_IPC.appln_id)\n",
    "    .join(TLS209_APPLN_IPC_2, TLS201_APPLN.appln_id == TLS209_APPLN_IPC_2.appln_id)\n",
    "    .filter(TLS201_APPLN.docdb_family_id.in_(intersection_docdb_family_ids))\n",
    "    .filter(TLS201_APPLN.earliest_filing_year.between(2010, 2022))\n",
    "    .filter(and_(\n",
    "        TLS209_APPLN_IPC.ipc_class_symbol != TLS209_APPLN_IPC_2.ipc_class_symbol,\n",
    "        func.left(TLS209_APPLN_IPC.ipc_class_symbol, 8) != func.left(TLS209_APPLN_IPC_2.ipc_class_symbol, 8)\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Execute the query\n",
    "result = final_query.all()\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "df = pd.DataFrame(result, columns=['patent_family_id', 'earliest_filing_year', 'IPC_1', 'IPC_2'])\n",
    "\n",
    "# Ensure that IPC classification codes are strings before truncating\n",
    "df['IPC_1'] = df['IPC_1'].astype(str).apply(lambda x: x[:8])\n",
    "df['IPC_2'] = df['IPC_2'].astype(str).apply(lambda x: x[:8])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6980eff-788b-4a86-9700-aba179d49fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       patent_family_id  earliest_filing_year     IPC_1     IPC_2\n",
      "0              55530492                  2015  C22B   7  C22B   3\n",
      "1              55530492                  2015  C22B  59  C22B   3\n",
      "2              67817265                  2019  G06T   7  G06F  30\n",
      "3              67817265                  2019  G06N   3  G06F  30\n",
      "4              67817265                  2019  G06N   3  G06F  30\n",
      "...                 ...                   ...       ...       ...\n",
      "51310          84858887                  2022  C22C  38  C22C  33\n",
      "51311          84858887                  2022  C22C  38  C22C  33\n",
      "51312          84858887                  2022  C22C  38  C22C  33\n",
      "51313          84858887                  2022  C22C  38  C22C  33\n",
      "51314          84858887                  2022  C22C  38  C22C  33\n",
      "\n",
      "[51315 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm import aliased\n",
    "from sqlalchemy import and_, func\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the necessary imports for your database models\n",
    "from epo.tipdata.patstat.database.models import TLS209_APPLN_IPC, TLS201_APPLN\n",
    "\n",
    "# Alias TLS209_APPLN_IPC to avoid confusion\n",
    "TLS209_APPLN_IPC_2 = aliased(TLS209_APPLN_IPC)\n",
    "\n",
    "# Step 4: Filter the data according to the timeframe 2010 - 2022 and join with TLS209_APPLN_IPC\n",
    "final_query = (\n",
    "    db.query(\n",
    "        TLS201_APPLN.docdb_family_id.label('patent_family_id'),\n",
    "        TLS201_APPLN.earliest_filing_year.label('earliest_filing_year'),\n",
    "        TLS209_APPLN_IPC.ipc_class_symbol.label('IPC_1'),\n",
    "        TLS209_APPLN_IPC_2.ipc_class_symbol.label('IPC_2')\n",
    "    )\n",
    "    .join(TLS209_APPLN_IPC, TLS201_APPLN.appln_id == TLS209_APPLN_IPC.appln_id)\n",
    "    .join(TLS209_APPLN_IPC_2, TLS201_APPLN.appln_id == TLS209_APPLN_IPC_2.appln_id)\n",
    "    .filter(TLS201_APPLN.docdb_family_id.in_(intersection_docdb_family_ids))\n",
    "    .filter(TLS201_APPLN.earliest_filing_year.between(2010, 2022))\n",
    "    .filter(and_(\n",
    "        TLS209_APPLN_IPC.ipc_class_symbol > TLS209_APPLN_IPC_2.ipc_class_symbol,\n",
    "        func.left(TLS209_APPLN_IPC.ipc_class_symbol, 8) > func.left(TLS209_APPLN_IPC_2.ipc_class_symbol, 8)\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Execute the query\n",
    "result = final_query.all()\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "df = pd.DataFrame(result, columns=['patent_family_id', 'earliest_filing_year', 'IPC_1', 'IPC_2'])\n",
    "\n",
    "# Ensure that IPC classification codes are strings before truncating\n",
    "df['IPC_1'] = df['IPC_1'].astype(str).apply(lambda x: x[:8])\n",
    "df['IPC_2'] = df['IPC_2'].astype(str).apply(lambda x: x[:8])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a66b32-ed5e-41b4-b4d8-e11abb6eeefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows deleted: 29778\n",
      "          IPC_1     IPC_2  earliest_filing_year  count_of_families\n",
      "0      A01G  17  A01B  79                  2021                  1\n",
      "1      A01G  20  A01G   9                  2016                  1\n",
      "2      A01G  22  A01B  79                  2021                  1\n",
      "3      A01G  22  A01G  17                  2021                  1\n",
      "4      A01H   6  A01H   5                  2022                  1\n",
      "...         ...       ...                   ...                ...\n",
      "10589  H05B   6  B03C   1                  2015                  1\n",
      "10590  H05B   6  B03C   7                  2015                  1\n",
      "10591  H05B   6  B23K  26                  2015                  1\n",
      "10592  H05B   6  H01F  13                  2015                  1\n",
      "10593  H10N  35  H01M  10                  2017                  3\n",
      "\n",
      "[10594 rows x 4 columns]\n",
      "The grouped data has been saved to 'grouped_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is the DataFrame we have just created\n",
    "# Calculate the number of duplicated rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates()\n",
    "\n",
    "# Print the number of duplicated rows deleted\n",
    "print(f\"Number of duplicated rows deleted: {num_duplicates}\")\n",
    "\n",
    "# Group by IPC_1, IPC_2, and earliest_filing_year and count the number of families\n",
    "df_grouped = df_unique.groupby(['IPC_1', 'IPC_2', 'earliest_filing_year']).size().reset_index(name='count_of_families')\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(df_grouped)\n",
    "# Save the DataFrame to an Excel file\n",
    "df_grouped.to_excel(\"grouped_data.xlsx\", index=False)\n",
    "\n",
    "print(\"The grouped data has been saved to 'grouped_data.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b446c6bb-8696-4444-9021-8300659e2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows deleted: 29778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57145/3844695591.py:16: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scatter plot has been saved as 'scatter_plot_2012_2017.html'.\n",
      "The scatter plot has been saved as 'scatter_plot_2018_2023.html'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Assuming df is the DataFrame we have just created\n",
    "# Calculate the number of duplicated rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates()\n",
    "\n",
    "# Print the number of duplicated rows deleted\n",
    "print(f\"Number of duplicated rows deleted: {num_duplicates}\")\n",
    "\n",
    "# Normalize IPC codes to ensure consistent ordering\n",
    "df_unique['IPC_pair'] = df_unique.apply(lambda row: tuple(sorted([row['IPC_1'], row['IPC_2']])), axis=1)\n",
    "\n",
    "# Group by IPC_pair and earliest_filing_year and count the number of families\n",
    "df_grouped = df_unique.groupby(['IPC_pair', 'earliest_filing_year']).size().reset_index(name='count_of_families')\n",
    "\n",
    "# Split IPC_pair back into IPC_1 and IPC_2 for plotting\n",
    "df_grouped[['IPC_1', 'IPC_2']] = pd.DataFrame(df_grouped['IPC_pair'].tolist(), index=df_grouped.index)\n",
    "\n",
    "# Function to create and save scatter plot for a given year range with a threshold for count_of_families\n",
    "def create_scatter_plot(df, start_year, end_year, filename, threshold=5):\n",
    "    # Filter the data for the given year range\n",
    "    df_filtered = df[(df['earliest_filing_year'] >= start_year) & (df['earliest_filing_year'] <= end_year)]\n",
    "    \n",
    "    # Aggregate the counts across the year range\n",
    "    df_aggregated = df_filtered.groupby(['IPC_1', 'IPC_2']).agg({'count_of_families': 'sum'}).reset_index()\n",
    "    \n",
    "    # Apply the threshold\n",
    "    df_aggregated = df_aggregated[df_aggregated['count_of_families'] >= threshold]\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    fig = px.scatter(df_aggregated, \n",
    "                     x='IPC_1', \n",
    "                     y='IPC_2', \n",
    "                     size='count_of_families', \n",
    "                     color='count_of_families',\n",
    "                     labels={'IPC_1': 'IPC1 Codes', 'IPC_2': 'IPC2 Codes', 'count_of_families': 'Count of Families'},\n",
    "                     title=f'Scatter Plot of IPC Codes and Family Counts for {start_year}-{end_year} (Threshold: {threshold} Families)')\n",
    "    \n",
    "    # Update layout to resize the vertical axis to let all classification codes' legends appear\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            automargin=True,\n",
    "            tickmode='array',\n",
    "            tickvals=df_aggregated['IPC_2'].unique(),  # Use unique IPC_2 values as tick values\n",
    "            ticktext=df_aggregated['IPC_2'].unique()  # Ensure all IPC_2 values are shown\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save the plot as an HTML file\n",
    "    pyo.plot(fig, filename=filename)\n",
    "    print(f\"The scatter plot has been saved as '{filename}'.\")\n",
    "\n",
    "# Create scatter plots for the specified year ranges with a threshold of 5 families\n",
    "create_scatter_plot(df_grouped, 2012, 2017, 'scatter_plot_2012_2017.html', threshold=5)\n",
    "create_scatter_plot(df_grouped, 2018, 2023, 'scatter_plot_2018_2023.html', threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61384b5-0d64-4fbf-a5c6-81b2d6586f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
